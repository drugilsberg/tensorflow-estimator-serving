{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensor2tensor.layers import common_layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## path and parameters definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKSPACE_PATH = '/tmp/tes-workspace'\n",
    "os.makedirs(WORKSPACE_PATH, exist_ok=True)\n",
    "DATA_PATH = os.path.join(WORKSPACE_PATH, 'data')\n",
    "os.makedirs(DATA_PATH, exist_ok=True)\n",
    "TRAIN_FILEPATH = os.path.join(DATA_PATH, 'train.txt')\n",
    "EVAL_FILEPATH = os.path.join(DATA_PATH, 'eval.txt')\n",
    "MAX_LEN = 200\n",
    "MODEL_DIR = os.path.join(WORKSPACE_PATH, 'test_model')\n",
    "MODEL_EXPORTER_DIR = '{}_exporter'.format(MODEL_DIR)\n",
    "TRAIN_STEPS = 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare data\n",
    "\n",
    "Here we use the pre-processed data from `tf.keras.datasets.imdb`. We dump them to disk in order to demonstrate how to use TensorFlow high-level API using custom files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data\n",
    "def _pad_and_write(X, y, max_len, filepath):\n",
    "    def _pad(a_list):\n",
    "        length = len(a_list)\n",
    "        padded = a_list\n",
    "        if length < max_len:\n",
    "            padding = max_len - length\n",
    "            padded =  a_list + [0]*padding\n",
    "        return padded\n",
    "            \n",
    "    with open(filepath, 'w') as fp:\n",
    "        fp.writelines([\n",
    "            '{}|{}\\n'.format(\n",
    "                ','.join(map(str,_pad(tokens))),\n",
    "                label\n",
    "            )\n",
    "            for tokens, label in zip(X,y)\n",
    "        ])\n",
    "   \n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.imdb.load_data(\n",
    "    path='imdb.npz',\n",
    "    num_words=None,\n",
    "    skip_top=0,\n",
    "    maxlen=MAX_LEN,\n",
    "    seed=113,\n",
    "    start_char=1,\n",
    "    oov_char=2,\n",
    "    index_from=3\n",
    ")\n",
    "_pad_and_write(X_train, y_train, MAX_LEN, TRAIN_FILEPATH)\n",
    "_pad_and_write(X_test, y_test, MAX_LEN, EVAL_FILEPATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define dataset\n",
    "\n",
    "Data are feeded to the model starting from a `tf.data.TextLineDataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dataset\n",
    "def _process_line(line):\n",
    "    \"\"\"Process a line to create input tensors and labels.\"\"\"\n",
    "    line_split = tf.string_split([line], '|')\n",
    "\n",
    "    return (\n",
    "        {\n",
    "            'tokens': tf.reshape(\n",
    "                tf.string_to_number(\n",
    "                    tf.string_split([line_split.values[0]], ',').values,\n",
    "                    out_type=tf.int32\n",
    "                ),\n",
    "                [MAX_LEN]\n",
    "            )\n",
    "        },\n",
    "        tf.string_to_number(line_split.values[1], out_type=tf.int32)\n",
    "    )\n",
    "\n",
    "\n",
    "def generate_dataset(filepath):\n",
    "    \"\"\"Generate a tf.Dataset given a filepath containing one observation per line.\"\"\"\n",
    "    return tf.data.TextLineDataset(\n",
    "        filepath\n",
    "    ).map(\n",
    "        lambda line: _process_line(line)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define model function\n",
    "\n",
    "Here we define a dummy model for text classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "def model_fn(\n",
    "    features, labels, mode, params\n",
    "):\n",
    "    \"\"\"\n",
    "    Model function definition.\n",
    "    \n",
    "    The model is quite dummy but is used for demonstration\n",
    "    purposes.\n",
    "    \"\"\"\n",
    "    is_training = mode == tf.estimator.ModeKeys.TRAIN\n",
    "    # NOTE: dropout considered only during training\n",
    "    dropout = 0.0\n",
    "    if is_training:\n",
    "        dropout = params.get('dropout', 0.0)\n",
    "    \n",
    "    tokens = features['tokens']\n",
    "\n",
    "    embedded_tokens = common_layers.embedding(\n",
    "        tokens,\n",
    "        params['vocabulary_size'],\n",
    "        params['embedding_size'],\n",
    "        name='embedding'\n",
    "    )\n",
    "\n",
    "    convolutions = common_layers.conv1d(\n",
    "        embedded_tokens, params['filters'], params['kernel_size']\n",
    "    )\n",
    "    \n",
    "    flattened = tf.layers.flatten(convolutions)\n",
    "    hidden = tf.layers.dropout(\n",
    "        tf.layers.dense(\n",
    "            flattened, params['dense_hidden_size'],\n",
    "            activation=params.get('activation', tf.nn.relu),\n",
    "            name='dense_hidden'\n",
    "        ),\n",
    "        rate=dropout,\n",
    "        training=is_training,\n",
    "        name='dropout_dense_hidden'\n",
    "    )\n",
    "    \n",
    "    \n",
    "    logits = tf.layers.dense(\n",
    "        hidden, params['number_of_classes'], name='logits'\n",
    "    )\n",
    "    predictions = tf.argmax(logits, axis=1)\n",
    "    \n",
    "    prediction_dict = {\n",
    "        'predictions': predictions\n",
    "    }\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            mode=mode, predictions=prediction_dict\n",
    "        )\n",
    "    \n",
    "    # loss\n",
    "    one_hot_labels = tf.one_hot(labels, depth=params['number_of_classes'])\n",
    "    cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "        labels=one_hot_labels, logits=logits\n",
    "    )\n",
    "    loss = tf.reduce_mean(cross_entropy)\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        learning_rate = tf.train.exponential_decay(\n",
    "            params.get('learning_rate', 0.001),\n",
    "            tf.train.get_global_step(),\n",
    "            decay_steps=params.get('decay_steps', 3000),\n",
    "            decay_rate=params.get('decay_rate', 0.96)\n",
    "        )\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            mode=mode, predictions=predictions, loss=loss,\n",
    "            train_op=optimizer.minimize(loss, tf.train.get_global_step())\n",
    "        )\n",
    "    if mode == tf.estimator.ModeKeys.EVAL:\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            mode=mode, loss=loss\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define functions used to feed data in train/eval/serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning functions\n",
    "def train_input_fn(params):\n",
    "    \"\"\"Train function given params.\"\"\"\n",
    "    batch_size = params['batch_size']\n",
    "    filepath = params['train_filepath']\n",
    "    dataset = generate_dataset(\n",
    "        filepath\n",
    "    ).cache().shuffle(\n",
    "        buffer_size=params.get('buffer_size', 50000)\n",
    "    ).batch(\n",
    "        batch_size, drop_remainder=True\n",
    "    ).repeat()\n",
    "    features, labels = dataset.make_one_shot_iterator().get_next()\n",
    "    return features, labels\n",
    "\n",
    "\n",
    "def eval_input_fn(params):\n",
    "    \"\"\"Eval function given params.\"\"\"\n",
    "    batch_size = params['batch_size']\n",
    "    filepath = params['eval_filepath']\n",
    "    dataset = generate_dataset(\n",
    "        filepath\n",
    "    ).batch(\n",
    "        batch_size, drop_remainder=True\n",
    "    )\n",
    "    features, labels = dataset.make_one_shot_iterator().get_next()\n",
    "    return features, labels\n",
    "\n",
    "\n",
    "def serving_input_with_params_fn(params):\n",
    "    \"\"\"Serving function given params.\"\"\"\n",
    "    features = {\n",
    "        'tokens': tf.placeholder(\n",
    "            tf.int32,\n",
    "            [None, MAX_LEN]\n",
    "        )\n",
    "    }\n",
    "    return tf.estimator.export.ServingInputReceiver(features, features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define parameters and model exporter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "params = {\n",
    "    'batch_size': 32,\n",
    "    'train_filepath': TRAIN_FILEPATH,\n",
    "    'eval_filepath': EVAL_FILEPATH,\n",
    "    'vocabulary_size': 88585,\n",
    "    'embedding_size': 64,\n",
    "    'kernel_size': 5,\n",
    "    'filters': 8,\n",
    "    'dense_hidden_size': 16,\n",
    "    'number_of_classes': 2,\n",
    "    'dropout': 0.75\n",
    "}\n",
    "# define exporter\n",
    "exporter = tf.estimator.LatestExporter(\n",
    "    MODEL_EXPORTER_DIR,\n",
    "    lambda: serving_input_with_params_fn(params),\n",
    "    exports_to_keep=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define a `tf.estimator.Estimator`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the estimator\n",
    "estimator = tf.estimator.Estimator(\n",
    "    model_fn=model_fn,\n",
    "    model_dir=MODEL_DIR,\n",
    "    params=params\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define training and eval specifications\n",
    "train_spec = tf.estimator.TrainSpec(\n",
    "   input_fn=train_input_fn,\n",
    "   max_steps=TRAIN_STEPS\n",
    ")\n",
    "eval_spec = tf.estimator.EvalSpec(\n",
    "    input_fn=eval_input_fn,\n",
    "    throttle_secs=60,\n",
    "    exporters=exporter\n",
    ")\n",
    "tf.estimator.RunConfig.save_checkpoints_steps = min(300, TRAIN_STEPS // 3)\n",
    "tf.estimator.RunConfig.save_checkpoints_secs = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the training on `tensorboard`\n",
    "print(\n",
    "    'Follow training evolution with:\\ntensorboard --logdir {}\\n'.format(estimator.model_dir)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and evaluate\n",
    "tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test the exported model with `tensorflow/serving` Docker image\n",
    "\n",
    "Here we assume that `docker` and `curl` are installed.\n",
    "Before running this section make sure the `tensorflow/serving` image is pulled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    'Serve the model with:\\ndocker run -p 8501:8501 '\n",
    "    '--mount type=bind,source={},target=/models/tes '\n",
    "    '-e MODEL_NAME=tes -t tensorflow/serving\\n'.format(exporter.name)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare query\n",
    "eval_data = pd.read_csv(EVAL_FILEPATH, sep='|', names=['tokens', 'label'])\n",
    "index = 1\n",
    "tokens = list(map(int, eval_data['tokens'][index].split(',')))\n",
    "\n",
    "query = json.dumps({ 'instances': [{\n",
    "    'tokens': tokens\n",
    "}]}, indent=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query the model deployed\n",
    "json.loads(subprocess.check_output(\n",
    "    ['curl', '-d', '{}'.format(query), 'http://localhost:8501/v1/models/tes:predict']\n",
    "))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-estimator-serving",
   "language": "python",
   "name": "tensorflow-estimator-serving"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
